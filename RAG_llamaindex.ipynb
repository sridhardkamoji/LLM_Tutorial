{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a43ba30f-ec72-4dc5-8d5f-a11e265f5f37",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4b6297-b29b-4e06-a683-606ca50545b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563b3c69-822c-40e4-b44e-100a45e06eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sridhar Kamoji\\Python\\RAG\\.env-rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import ServiceContext, StorageContext, load_index_from_storage\n",
    "from llama_index.core.node_parser import SentenceSplitter, SimpleNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637bc16-6c6e-49b2-9b2e-0e518f9e1100",
   "metadata": {},
   "source": [
    "### Load HF Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df594ee4-46d7-4c4c-ba92-87cf4c380026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_TOKEN = open('./HF-Token.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb9696-bee2-4649-9f77-a5a1b58d9695",
   "metadata": {},
   "source": [
    "### Load and Create Indexed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8d7316-325e-400b-abfc-8b0c9e18618d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|████████████████████████████████████████████████████████████████████| 677/677 [00:00<00:00, 70.9kB/s]\n",
      "C:\\Users\\Sridhar Kamoji\\Python\\RAG\\.env-rag\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sridhar Kamoji\\Python\\RAG\\embed_model\\models--mixedbread-ai--mxbai-embed-large-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 670M/670M [00:37<00:00, 17.9MB/s]\n",
      "tokenizer_config.json: 100%|███████████████████████████████████████████████████████| 1.24k/1.24k [00:00<00:00, 483kB/s]\n",
      "vocab.txt: 100%|████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 16.5MB/s]\n",
      "tokenizer.json: 100%|████████████████████████████████████████████████████████████████| 711k/711k [00:01<00:00, 690kB/s]\n",
      "special_tokens_map.json: 100%|█████████████████████████████████████████████████████████| 695/695 [00:00<00:00, 319kB/s]\n",
      "C:\\Users\\Sridhar Kamoji\\AppData\\Local\\Temp\\ipykernel_6912\\4241363072.py:10: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm = llm, embed_model=embed_model, chunk_size=1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the files from ./data folder. files present ['Data Science in Production Building Scalable Model Pipelines with Python by Ben G Weber (z-lib.org).pdf']\n",
      "number of documents -> 234 :: number of nodes -> len(nodes)\n",
      "sample document :: \n",
      " Doc ID: 8ee01dcd-e66d-471f-bf52-44fbd6218eaa\n",
      "Text: 196 7 Cloud Dataflow for Batch Modeling     FIGURE 7.5: Running\n",
      "the managed pipeline with autoscaling. source, which can take a\n",
      "significant amount of time for libraries such as Pandas. T o avoid\n",
      "lengthy startup delays, it’s helpful to avoid including libraries in\n",
      "the requirements file that are already included in the Dataflow SDK1.\n",
      "F or example,...\n",
      "##################################################\n",
      "sample node :: \n",
      " Node ID: f58e8ae7-c969-48d2-85c8-ffbe2991c0c7\n",
      "Text: 7.2 Batch Model Pipeline 195 command line argument, as shown\n",
      "below. The project parameter is needed to read and write data with\n",
      "BigQuery . After running the pipeline, you can validate that the\n",
      "workflow was successful by navigating to the BigQuery UI and checking\n",
      "for data in the destination table, as shown in Figure 7.4 . T o run\n",
      "the pipeline on ...\n"
     ]
    }
   ],
   "source": [
    "PERSIST_DIR = \"./storage\"\n",
    "data_folder = \"./data\"\n",
    "\n",
    "# initialize the service context LLM and embedding model\n",
    "\n",
    "# using huggingface's inference api for generating the answer based on retrieved docs\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "        model_name=\"HuggingFaceH4/zephyr-7b-beta\", token=HF_TOKEN, num_output=1024)\n",
    "\n",
    "# loading embedding model ref {https://huggingface.co/spaces/mteb/leaderboard} for available list of models\n",
    "embed_model = HuggingFaceEmbedding(model_name= \"mixedbread-ai/mxbai-embed-large-v1\", cache_folder='./embed_model')\n",
    "\n",
    "# create a service context by plugging in llm and embedding model that is passed as an argument during creating / loading the index data \n",
    "service_context = ServiceContext.from_defaults(llm = llm, embed_model=embed_model, chunk_size=1024)\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    \n",
    "    files = os.listdir(data_folder)\n",
    "    print(f\"Reading the files from {data_folder} folder. files present {files}\")\n",
    "    \n",
    "    documents = SimpleDirectoryReader(data_folder).load_data()\n",
    "    parser = SimpleNodeParser()\n",
    "    nodes = parser.get_nodes_from_documents(documents)\n",
    "    \n",
    "    print(f\"number of documents -> {len(documents)} :: number of nodes -> len(nodes)\")\n",
    "    print(f\"sample document :: \\n {documents[random.randint(0, len(documents))]}\")\n",
    "    print(50 * \"#\")\n",
    "    print(f\"sample node :: \\n {nodes[random.randint(0, len(nodes))]}\")\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    index = VectorStoreIndex(nodes=nodes, service_context=service_context, storage_context=storage_context)\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "    query_engine = index.as_query_engine()\n",
    "    \n",
    "else:\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context, service_context = service_context)\n",
    "    query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2365bb-2fed-46ab-97c1-bd0155e84bd1",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fe5db32-dfae-44f1-89b4-8818bccd5598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The book focuses on building predictive model services for product teams, and aims to provide data scientists with a set of tools to build scalable model pipelines using Python. It assumes prior knowledge of Python and Pandas, as well as some experience with modeling packages such as scikit-learn. The book covers a range of topics, including data ingestion, data cleaning, feature engineering, model training, model evaluation, model deployment, and model monitoring. It also introduces tools and cloud environments commonly used in industry settings, such as AWS SageMaker, Google Cloud AI Platform, and Kubernetes. The book emphasizes the importance of version control, testing, and documentation in data science workflows, and provides guidance on how to implement these principles using Git, Docker, and Jupyter Notebooks. Overall, the book aims to provide a practical, hands-on approach to data science, with a focus on building data products for product teams.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"what data science principles are outlined in this book?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb956a6-7e9d-4602-bd5b-222bc9eb269a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For high sparse data, some of the best models that you can use are:\n",
      "\n",
      "1. Random Forest: This is an ensemble learning method that uses multiple decision trees to make a prediction. It can handle high sparse data as it can handle missing values and noisy data.\n",
      "\n",
      "2. Gradient Boosting: This is another ensemble learning method that uses multiple weak learners to make a prediction. It can handle high sparse data as it can handle missing values and noisy data.\n",
      "\n",
      "3. XGBoost: This is an optimized distributed gradient boosting library that can handle high sparse data. It is designed to be fast and efficient, making it a popular choice for big data applications.\n",
      "\n",
      "4. LightGBM: This is a gradient boosting framework that can handle high sparse data. It uses a tree-based learning algorithm and is known for its speed and accuracy.\n",
      "\n",
      "5. Deep Learning Models: These are neural network models that can handle high sparse data. They can learn complex relationships between features and can handle missing values and noisy data. However, they require a lot of computational resources and may not be practical for very large datasets.\n",
      "\n",
      "It's essential to choose the right model based on the specific characteristics of your data and the problem you're trying to solve.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"what are best models that I can use if I have a high sparse data?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e7b98b2-c8f2-4554-8bd9-0705bad528ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A random forest is an ensemble learning method for classification, regression, and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random forest is an extension of the bagging meta-algorithm, which builds multiple decision trees at training time (also known as base learners), and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. In random forest, each decision tree is constructed using a random vector, where each element is selected from the training set. This is the main difference between bagging and random forest. Random forest is a supervised learning algorithm, which means it requires labeled data for training. The random forest algorithm is used for both classification and regression tasks. In classification tasks, the output of the random forest is the class label with the highest probability. In regression tasks, the output is a real number (continuous output). Random forest is a powerful algorithm for handling overfitting, as it constructs multiple decision trees and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. This helps to reduce the variance of the model and improve its generalization performance. In summary, a random forest is an ensemble learning method that constructs multiple decision trees at training time and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is an extension of the bagging meta-algorithm that uses a random vector to construct each decision tree. Random forest is a supervised learning algorithm that is used for both classification and regression tasks, and it helps to reduce overfitting by constructing multiple decision trees and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"explain random forest model\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc35df06-1dca-4c88-bb6a-ec47ddaef4da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To create a pipeline using Spark, follow these steps:\n",
      "\n",
      "1. Stage your input data in a distributed storage layer, such as S3.\n",
      "2. Load the data into a Spark DataFrame using Spark's data source APIs.\n",
      "3. Preprocess the data as needed, such as cleaning, transforming, or feature engineering.\n",
      "4. Split the data into training and testing sets.\n",
      "5. Train a machine learning model using Spark MLlib or another library.\n",
      "6. Evaluate the model's performance on the testing set.\n",
      "7. Save the model to persistent storage, such as S3 or a database.\n",
      "8. Load the saved model into a production environment and use it to make predictions on new data.\n",
      "\n",
      "In summary, creating a pipeline using Spark involves staging data, loading it into Spark, preprocessing it, splitting it into training and testing sets, training a model, evaluating it, saving it, and loading it into production.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"how can i use spark to create a pipeline?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b7cb41-c77f-4e14-b920-3ee24296d098",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The book mentions three types of Spark deployments:\n",
      "\n",
      "1. Self-hosted: An engineering team manages a set of clusters and provides console and notebook access.\n",
      "\n",
      "2. Cloud solutions: AWS EMR and GCP Cloud Dataproc are mentioned as examples.\n",
      "\n",
      "3. Vendor solutions: Databricks, Cloudera, and other vendors provide fully-managed Spark environments.\n",
      "\n",
      "The author recommends using a freely-available notebook environment for getting up and running with Spark as quickly as possible, especially for data scientists. The author also mentions that as the size of the team using Spark scales, additional considerations such as multi-tenancy and isolation become important, and self-hosted solutions require significant engineering work to support these features. Many organizations use cloud or vendor solutions for Spark due to these factors.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"what are the spark deployments mentioned in the book\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c341d2d0-0da1-407e-8e3c-ac45c7020b41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The book talks about three types of coding environments for writing Python code for data science: IDEs, text editors, and notebooks. The author recommends using notebook environments for exploratory analysis and productizing models, and text editors for building web applications with Flask and Dash. The author also mentions collaborative note-books in Databricks and Google Colab, and suggests sharing notebooks in version control systems like GitHub for collaboration. The author recommends working on a remote machine like EC2 to gain experience with cloud environments and setting up Python environments outside of the local machine.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"what are the coding environments that the book talks about?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9155cbd-1c4b-495b-a9f7-2ac8dc6966f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here's an example of how to implement logistic regression in Python using the scikit-learn library:\n",
      "\n",
      "```python\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, roc_auc_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Load the dataset\n",
      "data = pd.read_csv('dataset.csv')\n",
      "\n",
      "# Preprocess the data (e.g., encoding categorical variables)\n",
      "X = data.drop('target', axis=1)\n",
      "y = data['target']\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Fit the logistic regression model to the training data\n",
      "model = LogisticRegression()\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Evaluate the model on the testing data\n",
      "y_pred = model.predict(X_test)\n",
      "\n",
      "# Calculate the accuracy and ROC AUC score\n",
      "accuracy = accuracy_score(y_test, y_pred)\n",
      "auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
      "\n",
      "print(\"Accuracy:\", accuracy)\n",
      "print(\"ROC AUC:\", auc)\n",
      "```\n",
      "\n",
      "In this example, we first import the necessary modules from scikit-learn. We then load the dataset, preprocess it, and split it into training and testing sets using the `train_test_split` function. We then fit the logistic regression model to the training data using the `fit` method of the `LogisticRegression` class. We evaluate the model on the testing data using the `predict` method, and calculate the accuracy and ROC AUC score using the `accuracy_score` and `roc_auc_score` functions, respectively. Finally, we print the results.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"give me python code for logistic regression ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8872d2-84f2-485b-aa94-468d3d71a091",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To build a neural network for predicting the likelihood of purchasing a game using Keras, follow these steps:\n",
      "\n",
      "1. Import the required libraries:\n",
      "\n",
      "```python\n",
      "import tensorflow as tf\n",
      "import keras\n",
      "from keras import models, layers\n",
      "import matplotlib.pyplot as plt\n",
      "```\n",
      "\n",
      "2. Define the network structure:\n",
      "\n",
      "```python\n",
      "model = models.Sequential()\n",
      "model.add(layers.Dense(64, activation='relu', input_shape=(10,)))\n",
      "model.add(layers.Dropout(0.1))\n",
      "model.add(layers.Dense(64, activation='relu'))\n",
      "model.add(layers.Dense(1, activation='sigmoid'))\n",
      "```\n",
      "\n",
      "3. Compile the model:\n",
      "\n",
      "```python\n",
      "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[auc])\n",
      "```\n",
      "\n",
      "4. Define the `auc` metric for evaluating the model:\n",
      "\n",
      "```python\n",
      "def auc(y_true, y_pred):\n",
      "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
      "    keras.backend.get_session().run(tf.local_variables_initializer())\n",
      "    return auc\n",
      "```\n",
      "\n",
      "5. Train and evaluate the model:\n",
      "\n",
      "```python\n",
      "x_train, x_test, y_train, y_test = train_test_split(gamesDF.drop(['label'], axis=1), gamesDF['label'], test_size=0.3)\n",
      "model.fit(x_train, epochs=100, validation_data=(x_test, y_test))\n",
      "model.evaluate(x_test, y_test)\n",
      "```\n",
      "\n",
      "6. Save and load the model:\n",
      "\n",
      "```python\n",
      "model_path = \"models/logit_games_v1\"\n",
      "#shutil.rmtree(model_path)\n",
      "mlflow.sklearn.save_model(model, model_path)\n",
      "loaded = mlflow.sklearn.load_model(model_path)\n",
      "loaded.predict_proba(x)\n",
      "```\n",
      "\n",
      "This process is similar to the one described in Section 1.6.3, but instead of using the `LogisticRegression` class from scikit-learn, we're using Keras to build a neural network. The `auc` metric is defined to evaluate the model's performance, and the `save_model` and `load_model` functions from mlflow are used to persist the model for later use.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"give me the same using keras\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env-rag",
   "language": "python",
   "name": ".env-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
