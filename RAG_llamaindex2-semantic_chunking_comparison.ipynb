{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55691297-1b96-4af2-bf1b-e29dec682c03",
   "metadata": {},
   "source": [
    "# Comparing llama-index Semantic Chunking and SentenceSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ba30f-ec72-4dc5-8d5f-a11e265f5f37",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4b6297-b29b-4e06-a683-606ca50545b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563b3c69-822c-40e4-b44e-100a45e06eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sridhar Kamoji\\Python\\RAG\\.env-rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import ServiceContext, StorageContext, load_index_from_storage\n",
    "from llama_index.core.node_parser import SentenceSplitter, SimpleNodeParser, SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637bc16-6c6e-49b2-9b2e-0e518f9e1100",
   "metadata": {},
   "source": [
    "### Load HF Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df594ee4-46d7-4c4c-ba92-87cf4c380026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_TOKEN = open('./HF-Token.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba057add-cd38-4d5b-a8b6-94173c07c367",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8aa874-659e-4c85-a035-a1c04964295d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = \"./data\"\n",
    "documents = SimpleDirectoryReader(input_dir=data_folder).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9b1316a-2465-473a-92f4-3cf2736f2e50",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of documents :: 234\n",
      "####################################################################################################\n",
      "sample Documents ::\n",
      "####################################################################################################\n",
      "Ben G. W eber\n",
      "Data Science in Production:\n",
      "Building Scalable Model\n",
      "Pipelines with Python\n",
      "####################################################################################################\n",
      "8.2 Dataflow Streaming 215\n",
      "a callback pattern where you provide a function that is used to\n",
      "process messages as they arrive. In this example we simply print\n",
      "the data field in the message and then acknowledge that the mes-\n",
      "sage has been received. The for loop at the bottom of the code\n",
      "block is used to keep the script running, because none of the other\n",
      "commands suspend when executed.\n",
      "import time\n",
      "from google.cloud import pubsub_v1\n",
      "subscriber = pubsub_v1.SubscriberClient ()\n",
      "subscription_path = subscriber.subscription_path (\n",
      "\"your_project_name\" ,\"dsp\" )\n",
      "def callback (message):\n",
      "print (message.data)\n",
      "message.ack ()\n",
      "subscriber.subscribe (subscription_path, callback=callback)\n",
      "while True:\n",
      "time.sleep (10)\n",
      "W eâ€™ll use the same library to create a message producer in Python.\n",
      "The code snippet below shows how to use the Google Cloud library\n",
      "to create a publishing client, set up a connection to the topic, and\n",
      "publish a message to the dsp topic. F or the producer, we need to\n",
      "encode the message in utf-8 format before publishing the message.\n",
      "from google.cloud import pubsub_v1\n",
      "publisher = pubsub_v1.PublisherClient ()\n",
      "topic_path = publisher.topic_path (\"your_project_name8\" ,\"natality\" )\n",
      "data = \"Hello World!\" .encode ('utf-8' )\n",
      "publisher.publish (topic_path, data=data)\n",
      "####################################################################################################\n",
      "7.2 Batch Model Pipeline 193\n",
      "{'year' :2001, 'plurality' :1,'apgar_5min' :99, 'mother_age' :33,\n",
      "'father_age' :40, 'gestation_weeks' :38, 'ever_born' :8,\n",
      "'mother_married' :1,'weight' :6.8122838958,\n",
      "'time' :'2019-12-14 23:51:42.560931 UTC' ,\n",
      "'guid' :'b281c5e8-85b2-4cbd-a2d8-e501ca816363' }\n",
      "T o save the predictions to BigQuery , we need to define a schema\n",
      "that defines the structure of the predictions table. W e can do this\n",
      "using a utility function that converts a JSON description of the ta-\n",
      "ble schema into the schema object required by the Beam BigQuery\n",
      "writer. T o simplify the process, we can create a Python dictionary\n",
      "object and use the dumps command to generate JSON.\n",
      "schema = parse_table_schema_from_json (json.dumps ({'fields' :\n",
      "[ { 'name' :'guid' ,'type' :'STRING' },\n",
      "{'name' :'weight' ,'type' :'FLOAT64' },\n",
      "{'name' :'time' ,'type' :'STRING' } ]}))\n",
      "The next step is to create the pipeline and define a DAG of Beam\n",
      "operations. This time we are not providing input or output argu-\n",
      "ments to the pipeline, and instead we are passing the input and\n",
      "output destinations to the BigQuery operators. The pipeline has\n",
      "three steps: read from BigQuery , apply the model, and write to\n",
      "BigQuery . T o read from BigQuery , we pass in the query and spec-\n",
      "ify that we are using standard SQL. T o apply the model, we use\n",
      "our custom class for making predictions. T o write the results, we\n",
      "pass the schema and table name to the BigQuery writer, and spec-\n",
      "ify that a new table should be created if necessary and that data\n",
      "should be appended to the table if data already exists.\n",
      "# set up pipeline options\n",
      "parser = argparse.ArgumentParser ()\n",
      "known_args, pipeline_args = parser.parse_known_args (None)\n",
      "pipeline_options = PipelineOptions (pipeline_args)\n"
     ]
    }
   ],
   "source": [
    "print(f\"total number of documents :: {len(documents)}\")\n",
    "print(100*'#')\n",
    "print(\"sample Documents ::\")\n",
    "print(100*'#')\n",
    "print(documents[0].get_content())\n",
    "print(100*'#')\n",
    "print(documents[-10].get_content())\n",
    "print(100*'#')\n",
    "print(documents[random.randint(0, len(documents))].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93071b-453e-4928-80b2-1da3b53ff059",
   "metadata": {},
   "source": [
    "### Define Service Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54d70563-122a-4eef-a759-2b85b1fb65e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sridhar Kamoji\\AppData\\Local\\Temp\\ipykernel_14888\\3602237993.py:3: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm = llm, embed_model=embed_model, chunk_size=1024)\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name= \"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "        model_name=\"HuggingFaceH4/zephyr-7b-beta\", token=HF_TOKEN, num_output=1024)\n",
    "service_context = ServiceContext.from_defaults(llm = llm, embed_model=embed_model, chunk_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ab640-f0f5-413b-a338-816b52301743",
   "metadata": {},
   "source": [
    "### Define Sentence and Semantic chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8115af37-a056-4ef9-a68e-b0dd9012056a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "semantic_splitter = SemanticSplitterNodeParser(buffer_size=2, embed_model=embed_model, breakpoint_percentile_threshold=0.95)\n",
    "base_splitter = SentenceSplitter(chunk_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c70f089f-266e-484b-a5dd-a2cb981d56d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "semantic_nodes = semantic_splitter.get_nodes_from_documents(documents)\n",
    "base_nodes = base_splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39fb0a6-e608-4b36-b8b4-24030f931645",
   "metadata": {},
   "source": [
    "### Sample nodes and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c364ef66-ffab-4552-bbf2-5f6ef5b054df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ben G. W eber\\nData Science in Production:\\nBuilding Scalable Model\\nPipelines with Python'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Ben G. W eber\\nData Science in Production:\\nBuilding Scalable Model\\nPipelines with Python'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_nodes[0].get_content()\n",
    "base_nodes[0].get_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0622c003-5b62-4ad0-9ff0-7a03200b9ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contents\\nPreface vii\\n0.1 Prerequisites . '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Contents\\nPreface vii\\n0.1 Prerequisites . . . . . . . . . . . . . . . . . . . . vii\\n0.2 Book Contents . . . . . . . . . . . . . . . . . . . viii\\n0.3 Code Examples . . . . . . . . . . . . . . . . . . . x\\n0.4 Acknowledgements . . . . . . . . . . . . . . . . . x\\n1 Introduction 1\\n1.1 Applied Data Science . . . . . . . . . . . . . . . 3\\n1.2 Python for Scalable Compute . . . . . . . . . . . 4\\n1.3 Cloud Environments . . . . . . . . . . . . . . . . 6\\n1.3.1 Amazon W eb Services (A WS) . . . . . . . 7\\n1.3.2 Google Cloud Platform (GCP) . . . . . . 8\\n1.4 Coding Environments . . . . . . . . . . . . . . . 9\\n1.4.1 Jupyter on EC2 . . . . . . . . . . . . . . . 9\\n1.5 Datasets . . . . . . . . . . . .'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_nodes[2].get_content()\n",
    "base_nodes[2].get_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6f34c-a619-45ed-b41d-10fb718fea36",
   "metadata": {},
   "source": [
    "### Creating VectorStore Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "799e0152-b96b-4eed-9661-9fe7122814be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "semantic_index = VectorStoreIndex(semantic_nodes, service_context= service_context)\n",
    "base_index = VectorStoreIndex(base_nodes, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b6e4e71-bf81-4541-a7b2-6eccbbfa2001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "semantic_query_engine = semantic_index.as_query_engine()\n",
    "basic_query_engine = base_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2365bb-2fed-46ab-97c1-bd0155e84bd1",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5eb4bcb9-5716-49d3-bac2-ec89ed604ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_response(engine, qry):\n",
    "    response = engine.query(qry)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe35897e-c1a9-4713-aa62-e2531444ab1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The book outlines the discipline of applied data science and emphasizes the importance of quickly delivering a proof of concept and iteratively improving a model once it has been shown to provide value to an organization. It also discusses the use of Python and provides an overview of automated feature engineering. The book uses specific data sets, models, and cloud environments throughout its chapters.\n",
      "\n",
      "\n",
      "The book focuses on building predictive model services for product teams, and aims to provide data scientists with a set of tools to build scalable model pipelines using Python. It assumes prior knowledge of Python and Pandas, as well as some experience with modeling packages such as scikit-learn. The book covers a range of topics, including data ingestion, data cleaning, feature engineering, model training, model evaluation, model deployment, and model monitoring. It also introduces tools and cloud environments commonly used in industry settings, such as AWS SageMaker, Google Cloud AI Platform, and Kubernetes. The book emphasizes the importance of version control, testing, and documentation in data science workflows, and provides guidance on how to implement these principles using Git, Docker, and Jupyter Notebooks. Overall, the book aims to provide a practical, hands-on approach to data science, with a focus on building data products for product teams.\n"
     ]
    }
   ],
   "source": [
    "get_response(semantic_query_engine, \"what data science principles are outlined in this book?\")\n",
    "get_response(basic_query_engine, \"what data science principles are outlined in this book?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fb956a6-7e9d-4602-bd5b-222bc9eb269a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Random Forest: Random Forest is an ensemble learning method that uses multiple decision trees to make a prediction. It can handle high sparse data as it can handle missing values and noisy data.\n",
      "\n",
      "2. Gradient Boosting: Gradient Boosting is another ensemble learning method that combines multiple weak learners to make a strong prediction. It can handle high sparse data by iteratively improving the model's performance.\n",
      "\n",
      "3. XGBoost: XGBoost is an optimized distributed gradient boosting library that can handle high sparse data with its distributed computing capabilities.\n",
      "\n",
      "4. LightGBM: LightGBM is a gradient boosting framework that uses a gradient-based one-side sampling algorithm to handle high sparse data efficiently.\n",
      "\n",
      "5. CatBoost: CatBoost is a gradient boosting framework that uses a combination of gradient boosting and categorical feature boosting to handle high sparse data with categorical features.\n",
      "\n",
      "These models can be trained using Python libraries like scikit-learn, XGBoost, LightGBM, and CatBoost.\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "For high sparse data, you may want to consider using models that are specifically designed for handling sparse data, such as:\n",
      "\n",
      "1. Lasso Regression: This is a regularized linear regression algorithm that can handle sparse data by adding a penalty term to the objective function. This penalty term encourages the coefficients to be small, which can help to reduce the number of non-zero coefficients and make the model more sparse.\n",
      "\n",
      "2. Random Forest: This is an ensemble learning algorithm that can handle sparse data by using decision trees as base learners. Decision trees are inherently sparse, as they only consider a subset of features at each node. Random Forest can also handle missing values by replacing them with the mean or median of the feature.\n",
      "\n",
      "3. Gradient Boosting: This is another ensemble learning algorithm that can handle sparse data by using decision trees as base learners. Gradient Boosting can also handle missing values by replacing them with the mean or median of the feature.\n",
      "\n",
      "4. XGBoost: This is a distributed gradient boosting framework that can handle sparse data by using a sparse gradient computation algorithm. XGBoost can also handle missing values by replacing them with the mean or median of the feature.\n",
      "\n",
      "5. LightGBM: This is a gradient boosting framework that can handle sparse data by using a sparse gradient computation algorithm. LightGBM can also handle missing values by replacing them with the mean or median of the feature.\n",
      "\n",
      "These models can be implemented using libraries such as scikit-learn, XGBoost, and LightGBM. The choice of which model to use will depend on the specific characteristics of your data and the problem you are trying to solve. It's always a good idea to test multiple models on a small subset of your data to see which one performs best before deploying it in production.\n"
     ]
    }
   ],
   "source": [
    "get_response(semantic_query_engine, \"what are best models that I can use if I have a high sparse data?\")\n",
    "print(100*\"#\")\n",
    "get_response(basic_query_engine, \"what are best models that I can use if I have a high sparse data?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e7b98b2-c8f2-4554-8bd9-0705bad528ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A random forest is an ensemble learning method that combines multiple decision trees to improve the accuracy and robustness of predictions. Each decision tree is trained on a random subset of features and a random subset of training data, which helps to reduce overfitting and improve the model's ability to handle noisy and irrelevant features. The final prediction is made by averaging the predictions of all the decision trees in the forest. Random forests can be used for both classification and regression tasks, and are particularly effective for handling high-dimensional data with many features. In the context of the provided text, the random forest model is being used for regression to predict weights based on various input features. After searching through the parameter space and using cross validation to select the best hyperparameters, the model is retrained on the complete training data set and applied to make predictions on the test data set. The VectorAssembler transformer is used to create a vector representation of the input features, which is then fed into the random forest model for prediction.\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "A random forest is an ensemble learning method for classification, regression, and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random forest is an extension of the bagging meta-algorithm, which builds multiple decision trees at training time (also known as base learners), and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. In random forest, each decision tree is constructed using a random vector, where each element is selected from the training set. This is the main difference between bagging and random forest. Random forest is a supervised learning algorithm, which means it requires labeled data for training. The random forest algorithm is used for both classification and regression tasks. In classification tasks, the output of the random forest is the class label with the highest probability. In regression tasks, the output is a real number (continuous output). Random forest is a powerful algorithm for handling overfitting, as it constructs multiple decision trees and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. This helps to reduce the variance of the model and improve its generalization performance. In summary, a random forest is an ensemble learning method that constructs multiple decision trees at training time and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is an extension of the bagging meta-algorithm that uses a random vector to construct each decision tree. Random forest is a supervised learning algorithm that is used for both classification and regression tasks, and it helps to reduce overfitting by constructing multiple decision trees and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n"
     ]
    }
   ],
   "source": [
    "get_response(semantic_query_engine, \"explain random forest model\")\n",
    "print(100*\"#\")\n",
    "get_response(basic_query_engine, \"explain random forest model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc35df06-1dca-4c88-bb6a-ec47ddaef4da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To create a pipeline in Spark, you can follow these general steps:\n",
      "\n",
      "1. Define the stages of your pipeline, which can include data sources, transformations, and evaluations.\n",
      "2. Chain the stages together using the `Pipeline` class provided by Spark MLlib.\n",
      "3. Fit the pipeline to your training data using the `fit()` method.\n",
      "4. Transform your test data using the `transform()` method.\n",
      "5. Evaluate the performance of your pipeline using the `evaluate()` method.\n",
      "\n",
      "Here's an example of how to create a simple pipeline in Spark:\n",
      "\n",
      "```python\n",
      "from pyspark.ml.classification import LogisticRegression\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
      "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
      "\n",
      "# Load the data\n",
      "data = spark.createDataFrame(\n",
      "    pd.read_csv('path/to/data.csv', header=None),\n",
      "    schema=StructType([StructField('feature1', FloatType()),\n",
      "                       StructField('feature2', FloatType()),\n",
      "                       StructField('label', IntegerType())])\n",
      ")\n",
      "\n",
      "# Split the data into training and test sets\n",
      "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
      "\n",
      "# Define the feature vector assembly transform\n",
      "assembler = VectorAssembler(inputCols=['feature1', 'feature2'], outputCol='features')\n",
      "\n",
      "# Define the logistic regression model\n",
      "lr = LogisticRegression()\n",
      "\n",
      "# Define the pipeline with the assembly and logistic regression stages\n",
      "pipeline = Pipeline(stages=[assembler, lr])\n",
      "\n",
      "# Define the hyperparameter grid for cross-validation\n",
      "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01]).build()\n",
      "\n",
      "# Define the cross-validation object\n",
      "cv = CrossValidator(estimator=pipeline, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, numFolds=3)\n",
      "\n",
      "# Fit the cross-validator to the training data\n",
      "cvModel = cv.fit(train)\n",
      "\n",
      "# Transform the test data using the pipeline\n",
      "predictions = cvModel.transform(test)\n",
      "\n",
      "# Evaluate the performance of the pipeline on the test data\n",
      "auroc = predictions.select('probability', 'label').rdd.map(lambda row: (row.label, row.probability[0])).map(lambda pair: (1 if pair[1] > 0.5 else 0, pair[0])).countByValue().values().mean()\n",
      "print(\"Area Under ROC: \", auroc)\n",
      "```\n",
      "\n",
      "In this example, we first load the data and split it into training and test sets. We then define the feature vector assembly transform and the logistic regression model, and chain them together into a pipeline using the `Pipeline` class. We also define a hyperparameter grid for cross-validation using the `ParamGridBuilder` class. We then fit the cross-validator to the training data, transform the test data using the pipeline, and evaluate the performance of the pipeline using the `BinaryClassificationEvaluator` class. The `CrossValidator` class is used to perform multiple-fold cross-validation on the pipeline, which can help to improve the robustness and generalization performance of the model.\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "To create a pipeline using Spark, follow these steps:\n",
      "\n",
      "1. Stage your input data in a distributed storage layer, such as S3.\n",
      "2. Load the data into a Spark DataFrame using Spark's data source APIs.\n",
      "3. Preprocess the data as needed, such as cleaning, transforming, or feature engineering.\n",
      "4. Split the data into training and testing sets.\n",
      "5. Train a machine learning model using Spark MLlib or another library.\n",
      "6. Evaluate the model's performance on the testing set.\n",
      "7. Save the model to persistent storage, such as S3 or a database.\n",
      "8. Load the saved model into a production environment and use it to make predictions on new data.\n",
      "\n",
      "In summary, creating a pipeline using Spark involves staging data, loading it into Spark, preprocessing it, splitting it into training and testing sets, training a model, evaluating it, saving it, and loading it into production.\n"
     ]
    }
   ],
   "source": [
    "get_response(semantic_query_engine, \"how can i use spark to create a pipeline?\")\n",
    "#semantic engine performs better by giving example\n",
    "print(100*\"#\")\n",
    "get_response(basic_query_engine, \"how can i use spark to create a pipeline?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24b7cb41-c77f-4e14-b920-3ee24296d098",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Self Hosted: An engineering team manages a set of clusters and provides console and notebook access.\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "The book mentions three types of Spark deployments:\n",
      "1. Self-hosted: An engineering team manages a set of clusters and provides console and notebook access.\n",
      "2. Cloud solutions: Amazon Web Services (AWS) provides a managed Spark option called EMR, and Google Cloud Platform (GCP) has Cloud Dataproc.\n",
      "3. Vendor solutions: Databricks, Cloudera, and other vendors provide fully-managed Spark environments.\n",
      "\n",
      "The author recommends using a freely-available notebook environment for getting up and running with Spark as quickly as possible, especially for data scientists. The author also mentions that as the size of the team using Spark scales, additional considerations such as multi-tenancy and isolation become important, and self-hosted solutions require significant engineering work to support these considerations. Therefore, many organizations use cloud or vendor solutions for Spark. In this book, the author uses the Databricks Community Edition, which provides all the baseline features needed for learning Spark in a collaborative notebook environment.\n"
     ]
    }
   ],
   "source": [
    "get_response(semantic_query_engine, \"what are the spark deployments mentioned in the book\")\n",
    "# semantic response performs poor compared to basic engine\n",
    "print(100*\"#\")\n",
    "get_response(basic_query_engine, \"what are the spark deployments mentioned in the book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c341d2d0-0da1-407e-8e3c-ac45c7020b41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. IDEs (Integrated Development Environments)\n",
      "2. Text editors\n",
      "3. Notebooks (which are becoming more and more common as the place to write Python scripts)\n",
      "\n",
      "The book mentions that the best environment to use likely varies based on what you are building, but it provides an overview of three types of coding environments for Python: IDEs, text editors, and notebooks. Notebooks are becoming increasingly popular as a place to write Python scripts for data science.\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "The book talks about three types of coding environments for writing Python code for data science: IDEs, text editors, and notebooks. The author recommends using notebook environments for exploratory analysis and productizing models, and text editors for building web applications with Flask and Dash. The author also mentions collaborative note-books in Databricks and Google Colab, and suggests sharing notebooks in version control systems like GitHub for collaboration. The author recommends working on a remote machine like EC2 to gain experience with cloud environments and setting up Python environments outside of the local machine.\n"
     ]
    }
   ],
   "source": [
    "get_response(semantic_query_engine, \"what are the coding environments that the book talks about?\")\n",
    "print(100*\"#\")\n",
    "get_response(basic_query_engine, \"what are the coding environments that the book talks about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9155cbd-1c4b-495b-a9f7-2ac8dc6966f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here's an example of how to build a logistic regression model using scikit-learn in Python:\n",
      "\n",
      "```python\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('your_dataset.csv')\n",
      "\n",
      "# Preprocess the data as needed (e.g., one-hot encoding categorical features)\n",
      "X = df.drop('target', axis=1)\n",
      "y = df['target']\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
      "\n",
      "# Train the logistic regression model\n",
      "model = LogisticRegression()\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Evaluate the model on the test set\n",
      "y_pred = model.predict_proba(X_test)[:, 1]\n",
      "roc_auc = roc_auc_score(y_test, y_pred)\n",
      "print(\"ROC AUC:\", roc_auc)\n",
      "```\n",
      "\n",
      "In this example, we first import the necessary modules from scikit-learn. We then load the dataset, preprocess it as needed, and split it into training and testing sets using the `train_test_split` function. We then train the logistic regression model using the `fit` method of the `LogisticRegression` class, and evaluate it on the test set using the `predict_proba` method and the `roc_auc_score` function from scikit-learn's metrics module. The `predict_proba` method returns the predicted probabilities for each class, and we extract the probability of the positive class (i.e., the class we're interested in) using the `[:, 1]` indexing. The `roc_auc_score` function calculates the area under the receiver operating characteristic (ROC) curve, which is a common metric for evaluating binary classification models.\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Here's an example of how to implement logistic regression in Python using the scikit-learn library:\n",
      "\n",
      "```python\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, roc_auc_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Load the dataset\n",
      "data = pd.read_csv('dataset.csv')\n",
      "\n",
      "# Preprocess the data (e.g., encoding categorical variables)\n",
      "X = data.drop('target', axis=1)\n",
      "y = data['target']\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Fit the logistic regression model to the training data\n",
      "model = LogisticRegression()\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Evaluate the model on the testing data\n",
      "y_pred = model.predict(X_test)\n",
      "\n",
      "# Calculate the accuracy and ROC AUC score\n",
      "accuracy = accuracy_score(y_test, y_pred)\n",
      "auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
      "\n",
      "print(\"Accuracy:\", accuracy)\n",
      "print(\"ROC AUC:\", auc)\n",
      "```\n",
      "\n",
      "In this example, we first import the necessary modules from scikit-learn. We then load the dataset, preprocess it, and split it into training and testing sets using the `train_test_split` function. We then fit the logistic regression model to the training data using the `fit` method of the `LogisticRegression` class. We evaluate the model on the testing data using the `predict` method, and calculate the accuracy and ROC AUC score using the `accuracy_score` and `roc_auc_score` functions, respectively. Finally, we print the results.\n"
     ]
    }
   ],
   "source": [
    "get_response(semantic_query_engine, \"give me python code for logistic regression ?\")\n",
    "print(100*\"#\")\n",
    "get_response(basic_query_engine, \"give me python code for logistic regression ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f8872d2-84f2-485b-aa94-468d3d71a091",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To achieve the same using Keras, you can follow these steps:\n",
      "\n",
      "1. Import the required libraries:\n",
      "\n",
      "```python\n",
      "import tensorflow as tf\n",
      "import keras\n",
      "from keras import models, layers\n",
      "```\n",
      "\n",
      "2. Define the network structure:\n",
      "\n",
      "```python\n",
      "model = models.Sequential()\n",
      "model.add(layers.Dense(64, activation='relu', input_shape=(10,)))\n",
      "model.add(layers.Dropout(0.1))\n",
      "model.add(layers.Dense(64, activation='relu'))\n",
      "model.add(layers.Dense(1, activation='sigmoid'))\n",
      "```\n",
      "\n",
      "3. Compile the model:\n",
      "\n",
      "```python\n",
      "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[auc])\n",
      "\n",
      "def auc(y_true, y_pred):\n",
      "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
      "    keras.backend.get_session().run(tf.local_variables_initializer())\n",
      "    return auc\n",
      "```\n",
      "\n",
      "In this example, we're defining a custom metric called `auc` using the `tf.metrics.auc` function. This metric will be used to evaluate the model's performance during training and testing.\n",
      "\n",
      "4. Train and evaluate the model:\n",
      "\n",
      "```python\n",
      "model.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val), callbacks=[early_stopping])\n",
      "\n",
      "y_pred = model.predict(x_test)\n",
      "\n",
      "print(\"AUC:\", auc(y_test, y_pred))\n",
      "```\n",
      "\n",
      "In this example, we're using the `fit()` function to train the model for 100 epochs. We're also passing the `early_stopping` callback to stop the training process if the model's performance on the validation set doesn't improve for a certain number of epochs.\n",
      "\n",
      "After training, we're predicting the probabilities for the test set using the `predict()` function and calculating the AUC metric using the `auc()` function we defined earlier.\n",
      "\n",
      "5. Save and load the model:\n",
      "\n",
      "```python\n",
      "model_path = \"models/logit_games_v1\"\n",
      "shutil.rmtree(model_path)\n",
      "\n",
      "mlflow.sklearn.save_model(model, model_path)\n",
      "\n",
      "loaded = mlflow.sklearn.load_model(model_path)\n",
      "\n",
      "y_pred = loaded.predict(x_test)\n",
      "\n",
      "print(\"Loaded model AUC:\", auc(y_test, y_pred))\n",
      "```\n",
      "\n",
      "In this example, we're saving the trained model using the `mlflow.sklearn.save_model()` function. We're also deleting the existing model directory using the `shutil.rmtree()` function to ensure that the new model is saved in a clean directory.\n",
      "\n",
      "After loading the model using the `mlflow.sklearn.load_model()` function, we're predicting the probabilities for the test set and calculating the AUC metric using the `auc()` function we defined earlier.\n",
      "\n",
      "This process can be repeated for any Keras model to save and load it using MLflow.\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "To build a neural network for predicting the likelihood of purchasing a game using Keras, follow these steps:\n",
      "\n",
      "1. Import the required libraries:\n",
      "\n",
      "```python\n",
      "import tensorflow as tf\n",
      "import keras\n",
      "from keras import models, layers\n",
      "import matplotlib.pyplot as plt\n",
      "```\n",
      "\n",
      "2. Define the network structure:\n",
      "\n",
      "```python\n",
      "model = models.Sequential()\n",
      "model.add(layers.Dense(64, activation='relu', input_shape=(10,)))\n",
      "model.add(layers.Dropout(0.1))\n",
      "model.add(layers.Dense(64, activation='relu'))\n",
      "model.add(layers.Dense(1, activation='sigmoid'))\n",
      "```\n",
      "\n",
      "3. Compile the model:\n",
      "\n",
      "```python\n",
      "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[auc])\n",
      "```\n",
      "\n",
      "4. Define the `auc` metric for evaluating the model:\n",
      "\n",
      "```python\n",
      "def auc(y_true, y_pred):\n",
      "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
      "    keras.backend.get_session().run(tf.local_variables_initializer())\n",
      "    return auc\n",
      "```\n",
      "\n",
      "5. Train and evaluate the model:\n",
      "\n",
      "```python\n",
      "x_train, x_test, y_train, y_test = train_test_split(gamesDF.drop(['label'], axis=1), gamesDF['label'], test_size=0.3)\n",
      "model.fit(x_train, epochs=100, validation_data=(x_test, y_test))\n",
      "model.evaluate(x_test, y_test)\n",
      "```\n",
      "\n",
      "6. Save and load the model:\n",
      "\n",
      "```python\n",
      "model_path = \"models/logit_games_v1\"\n",
      "#shutil.rmtree(model_path)\n",
      "mlflow.sklearn.save_model(model, model_path)\n",
      "loaded = mlflow.sklearn.load_model(model_path)\n",
      "loaded.predict_proba(x)\n",
      "```\n",
      "\n",
      "This process is similar to the one described in Section 1.6.3, but instead of using the `LogisticRegression` class from scikit-learn, we're using Keras to build a neural network. The `auc` metric is defined to evaluate the model's performance, and the `save_model` and `load_model` functions from mlflow are used to persist the model for later use.\n"
     ]
    }
   ],
   "source": [
    "get_response(semantic_query_engine, \"give me the same using keras\")\n",
    "print(100*\"#\")\n",
    "get_response(basic_query_engine, \"give me the same using keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c1304-47aa-4f5b-a819-df676b13fc99",
   "metadata": {},
   "source": [
    "Although we can see Semantic performs well in some case by giving code examples but fails to give complete answers in other cases. <br>\n",
    "Nevertheless both are equally good in retrieving the relevant nodes from the indexed data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env-rag",
   "language": "python",
   "name": ".env-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
